# üöÄ Optimizaciones para Archivos Grandes - Prevenci√≥n de Fraudes

## ‚ùå Problema Original
El servidor se quedaba sin memoria (heap out of memory) al procesar archivos grandes de prevenci√≥n de fraudes, especialmente archivos con m√°s de 50,000 filas.

## ‚úÖ Soluciones Implementadas

### 1. **Optimizaci√≥n de Memoria Node.js**
```bash
# Windows
start-server-optimized.bat

# Linux/Mac  
./start-server-optimized.sh
```

**Configuraciones aplicadas:**
- `--max-old-space-size=4096`: Aumenta memoria heap a 4GB
- `--expose-gc`: Habilita garbage collection manual

### 2. **Streaming de Archivos Excel**
- **Antes**: Cargaba todo el archivo en memoria
- **Ahora**: Procesa fila por fila con streaming optimizado
- **Opciones de lectura optimizadas**:
  - `sharedStrings: 'cache'`
  - `hyperlinks: 'ignore'`
  - `styles: 'ignore'`

### 3. **Batch Processing Inteligente**
- **Archivos peque√±os** (<10k filas): Batch de 1000
- **Archivos medianos** (10k-50k): Batch de 750  
- **Archivos grandes** (>50k): Batch de 500
- **Transacciones de base de datos** para mejor performance

### 4. **Gesti√≥n de Memoria Autom√°tica**
- **Monitoreo cada 5000 filas**
- **Garbage collection forzado** cuando memoria > 1.5GB
- **Liberaci√≥n inmediata de batches** procesados
- **Limpieza autom√°tica** de archivos temporales

### 5. **Conexiones de BD Optimizadas**
- **Conexi√≥n dedicada** por upload
- **Sin timeouts** durante procesamiento
- **Rollback autom√°tico** en caso de error
- **Pool de conexiones** mejorado

## üìä L√≠mites Recomendados

| Tama√±o Archivo | Filas Aprox. | Memoria Usada | Tiempo Est. |
|---------------|--------------|---------------|-------------|
| 50 MB         | 50,000       | 1 GB          | 2-5 min     |
| 100 MB        | 100,000      | 2 GB          | 5-10 min    |
| 200 MB        | 200,000      | 3-4 GB        | 10-20 min   |

## üîß Configuraci√≥n de Render

Para producci√≥n en Render, aseg√∫rate de configurar:

```bash
# Variables de entorno en Render
NODE_OPTIONS=--max-old-space-size=4096 --expose-gc
```

## üö® Troubleshooting

### Si sigues viendo errores de memoria:

1. **Reduce el tama√±o del archivo**:
   - Dividir en m√∫ltiples archivos m√°s peque√±os
   - Procesar por lotes de 25,000-50,000 filas

2. **Aumentar memoria**:
   ```bash
   # Para archivos extremadamente grandes
   NODE_OPTIONS=--max-old-space-size=8192 --expose-gc
   ```

3. **Verificar formato del archivo**:
   - Eliminar columnas innecesarias
   - Convertir a CSV si es muy grande
   - Verificar que no haya f√≥rmulas complejas

### Monitoreo en tiempo real:
- Los logs muestran uso de memoria cada 10 batches
- El endpoint `/progreso` muestra avance en tiempo real
- Alertas autom√°ticas si memoria > 1.5GB

## üéØ Resultados Esperados

Con estas optimizaciones deber√≠as poder procesar:
- ‚úÖ Archivos de hasta 200MB sin problemas
- ‚úÖ M√°s de 100,000 filas de datos
- ‚úÖ Tiempo de procesamiento reducido 50-70%
- ‚úÖ Uso de memoria estable y controlado

## üìû Soporte

Si contin√∫as experimentando problemas:
1. Revisa los logs del servidor para detalles espec√≠ficos
2. Verifica el tama√±o y formato del archivo
3. Considera dividir archivos muy grandes en partes m√°s peque√±as
