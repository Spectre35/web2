import cvReadyPromise from '@techstark/opencv-js';
import sharp from 'sharp';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// ================= üîÑ PROCESADOR DE IM√ÅGENES CON OPENCV.JS =================

class ImageProcessor {
  constructor() {
    this.cv = null;
    this.isInitialized = false;

    // Configuraciones para detecci√≥n de orientaci√≥n
    this.rotationAngles = [0, 90, 180, 270];

    // Palabras clave espec√≠ficas de EUROPIEL para validaci√≥n
    this.europielKeywords = [
      'EUROPIEL', 'SINERGIA', 'LASER', 'CENTER', 'Recibo', 'Pago',
      'TRANSACCION', 'APROBADA', 'Fecha', 'cantidad', 'concepto',
      'Tarjeta', 'Autorizacion', 'Orden', 'Comercio', 'ARQC',
      'Folio', 'ANTICIPO', 'PAQUETE', 'NUEVO', 'Recibi'
    ];

    // Dimensiones aproximadas de un recibo individual (en p√≠xeles)
    this.receiptDimensions = {
      minWidth: 800,
      maxWidth: 1200,
      minHeight: 1000,
      maxHeight: 1600,
      aspectRatio: 0.77 // Proporci√≥n ancho/alto t√≠pica de un recibo
    };

    console.log('üéØ ImageProcessor inicializado - Usando @techstark/opencv-js');
  }

  /**
   * üöÄ Preprocesamiento espec√≠fico para recibos - Optimiza la imagen para m√°xima legibilidad OCR
   * @param {Buffer|string} input - Buffer de imagen o ruta de archivo
   * @returns {Promise<Buffer>} - Buffer de imagen optimizada
   */
  async optimizeForReceipts(input) {
    try {
      console.log('üîß Aplicando optimizaci√≥n espec√≠fica para recibos...');

      let imageBuffer;
      if (typeof input === 'string') {
        imageBuffer = await fs.readFile(input);
      } else {
        imageBuffer = input;
      }

      // üìä An√°lisis inicial de la imagen
      const metadata = await sharp(imageBuffer).metadata();
      console.log(`üìä Imagen original: ${metadata.width}x${metadata.height}, formato: ${metadata.format}`);

      // üéØ FASE 1: Escalado inteligente basado en el contenido
      const targetHeight = Math.min(Math.max(metadata.height, 2000), 3000); // Entre 2000-3000px
      const scaleFactor = targetHeight / metadata.height;
      
      let optimizedBuffer = await sharp(imageBuffer)
        .resize({
          height: targetHeight,
          width: Math.round(metadata.width * scaleFactor),
          kernel: sharp.kernel.lanczos3,
          fit: 'fill'
        })
        // Convertir a escala de grises de inmediato para mejor procesamiento
        .greyscale()
        // Normalizar el rango din√°mico
        .normalize()
        .toBuffer();

      // üéØ FASE 2: Correcci√≥n de iluminaci√≥n y contraste adaptativo
      const stats = await sharp(optimizedBuffer).stats();
      const avgBrightness = (stats.channels[0].mean / 255);
      
      // Ajustar gamma seg√∫n el brillo promedio (rango v√°lido: 1.0-3.0)
      let gamma = 1.0;
      if (avgBrightness < 0.4) gamma = 1.4; // Imagen muy oscura
      else if (avgBrightness > 0.7) gamma = 1.1; // Imagen muy clara (corregido: era 0.8)
      else gamma = 1.2; // Imagen normal

      optimizedBuffer = await sharp(optimizedBuffer)
        .gamma(gamma)
        .linear(1.3, -15) // Aumentar contraste, reducir offset
        .toBuffer();

      // üéØ FASE 3: Filtrado de ruido manteniendo los bordes del texto
      optimizedBuffer = await sharp(optimizedBuffer)
        .blur(0.5) // Blur muy sutil para reducir ruido de alta frecuencia
        .sharpen({ // Re-enfocar con par√°metros espec√≠ficos para texto
          sigma: 0.8,
          flat: 1.0,
          jagged: 1.5
        })
        .toBuffer();

      // üéØ FASE 4: Binarizaci√≥n adaptativa para texto claro
      const finalBuffer = await sharp(optimizedBuffer)
        .threshold(125, { // Umbral optimizado para recibos
          grayscale: false,
          greyscale: false
        })
        // Aplicar morfolog√≠a para limpiar el texto
        .convolve({ // Kernel para limpiar bordes
          width: 3,
          height: 3,
          kernel: [
            -1, -1, -1,
            -1,  9, -1,
            -1, -1, -1
          ]
        })
        .png() // Formato sin compresi√≥n
        .toBuffer();

      console.log('‚úÖ Optimizaci√≥n espec√≠fica para recibos completada');
      return finalBuffer;

    } catch (error) {
      console.error('‚ùå Error en optimizaci√≥n para recibos:', error.message);
      // Fallback a procesamiento b√°sico
      return await sharp(input)
        .greyscale()
        .normalize()
        .sharpen()
        .threshold(128)
        .png()
        .toBuffer();
    }
  }

  /**
   * Detecta y separa m√∫ltiples recibos usando an√°lisis simple de dimensiones
   * @param {string} imagePath - Ruta de la imagen original
   * @returns {Promise<Array>} Array de objetos con las rutas de los recibos detectados
   */
  async detectAndSeparateReceipts(imagePath) {
    try {
      console.log(`üîç DETECTAR M√öLTIPLES RECIBOS - Procesando archivo: ${imagePath}`);

      // Leer la imagen original
      const imageBuffer = await fs.readFile(imagePath);
      const metadata = await sharp(imageBuffer).metadata();

      console.log(`üìê DIMENSIONES DE IMAGEN: ${metadata.width}x${metadata.height}`);

      // Calcular proporci√≥n altura/ancho
      const aspectRatio = metadata.height / metadata.width;
      console.log(`üìè PROPORCI√ìN ALTURA/ANCHO: ${aspectRatio.toFixed(2)}`);
      console.log(`üéØ UMBRAL DE DETECCI√ìN: 1.25 (actual: ${aspectRatio.toFixed(2)})`);

      // üéØ DETECCI√ìN INTELIGENTE: Usar imageSegmentation para determinar n√∫mero real de recibos
      console.log('üîç INICIANDO DETECCI√ìN INTELIGENTE DE RECIBOS...');
      
      // Importar y usar imageSegmentation
      const ImageSegmentationModule = await import('./imageSegmentation.js');
      const ImageSegmentation = ImageSegmentationModule.default;
      const segmenter = new ImageSegmentation();
      
      // Detectar regiones reales de recibos
      const receiptRegions = await segmenter.detectReceiptRegions(imagePath, metadata.width, metadata.height);
      
      console.log(`üìä RESULTADO DETECCI√ìN: ${receiptRegions.length} recibo(s) detectado(s)`);
      
      if (receiptRegions.length === 1) {
        console.log('‚úÖ UN SOLO RECIBO DETECTADO - Procesando imagen completa');
        return [imagePath]; // Devolver imagen original sin dividir
      }
      
      if (receiptRegions.length >= 2) {
        console.log(`üîç ${receiptRegions.length} RECIBOS DETECTADOS - DIVIDIENDO INTELIGENTEMENTE...`);

        // Generar nombres √∫nicos para cada recibo
        const baseName = imagePath.replace(/\.[^/.]+$/, "");
        const receiptPaths = [];

        // Extraer cada regi√≥n detectada con m√°rgenes de seguridad
        for (let i = 0; i < receiptRegions.length; i++) {
          const region = receiptRegions[i];
          const receiptPath = `${baseName}_receipt_${i + 1}.png`;
          
          console.log(`üìÅ Extrayendo recibo ${i + 1}: ${receiptPath}`);
          console.log(`üîß Regi√≥n original: x=${region.x}, y=${region.y}, width=${region.width}, height=${region.height}`);
          
          // üéØ APLICAR M√ÅRGENES DE SEGURIDAD PARA EVITAR CORTAR TEXTO
          const marginTop = i === 0 ? 0 : 30;    // Sin margen superior para el primer recibo
          const marginBottom = 20;                // Margen inferior para todos
          const marginSides = 10;                 // M√°rgenes laterales peque√±os
          
          // Calcular nuevas coordenadas con m√°rgenes seguros
          const safeLeft = Math.max(0, region.x - marginSides);
          const safeTop = Math.max(0, region.y - marginTop);
          const safeWidth = Math.min(metadata.width - safeLeft, region.width + (2 * marginSides));
          const safeHeight = Math.min(metadata.height - safeTop, region.height + marginTop + marginBottom);
          
          console.log(`‚úÖ Regi√≥n con m√°rgenes: x=${safeLeft}, y=${safeTop}, width=${safeWidth}, height=${safeHeight}`);
          console.log(`üìè M√°rgenes aplicados: superior=${marginTop}px, inferior=${marginBottom}px, laterales=${marginSides}px`);

          await sharp(imageBuffer)
            .extract({
              left: safeLeft,
              top: safeTop,
              width: safeWidth,
              height: safeHeight
            })
            .png()
            .toFile(receiptPath);

          receiptPaths.push(receiptPath);
        }

        console.log(`‚úÇÔ∏è IMAGEN DIVIDIDA EN ${receiptRegions.length} RECIBOS EXITOSAMENTE`);
        console.log(`üéØ RETORNANDO ${receiptRegions.length} RECIBOS SEPARADOS`);
        
        // Crear resultado con formato correcto
        const result = receiptPaths.map((path, index) => ({
          path: path,
          index: index + 1,
          total: receiptPaths.length
        }));
        
        console.log(`üìã RESULTADO DE DIVISI√ìN:`, result);
        return result;
      }

      // Si no se detectaron m√∫ltiples recibos, procesar como √∫nico
      console.log('üìÑ PROCESANDO COMO RECIBO √öNICO (detecci√≥n inteligente confirm√≥ 1 recibo)');
      console.log(`üéØ RETORNANDO 1 RECIBO √öNICO con path: ${imagePath}`);
      
      // Asegurar que imagePath existe y es v√°lido
      if (!imagePath) {
        throw new Error('imagePath es undefined en detectAndSeparateReceipts');
      }
      
      const result = [{ path: imagePath, index: 1, total: 1 }];
      console.log(`üìã RESULTADO √öNICO:`, result);
      return result;

    } catch (error) {
      console.error('‚ùå ERROR DETECTANDO M√öLTIPLES RECIBOS:', error);
      console.error('‚ùå STACK TRACE:', error.stack);
      console.log('üîÑ FALLBACK: Retornando imagen original como recibo √∫nico');
      console.log(`üîÑ FALLBACK path: ${imagePath}`);
      
      // Asegurar que imagePath existe y es v√°lido
      if (!imagePath) {
        throw new Error('imagePath es undefined en detectAndSeparateReceipts fallback');
      }
      
      // En caso de error, devolver la imagen original
      return [{ path: imagePath, index: 1, total: 1 }];
    }
  }

  /**
   * Inicializar OpenCV.js
   */
  async initialize() {
    if (this.isInitialized) return;

    console.log('üöÄ Inicializando OpenCV.js desde NPM...');

    try {
      // Usar el paquete NPM oficial
      this.cv = await cvReadyPromise;

      console.log('‚úÖ OpenCV.js inicializado correctamente');
      console.log('üìã Build info:', this.cv.getBuildInformation().substring(0, 100) + '...');

      this.isInitialized = true;

    } catch (error) {
      console.error('‚ùå Error inicializando OpenCV.js:', error.message);
      throw new Error(`No se pudo inicializar OpenCV.js: ${error.message}`);
    }
  }



  /**
   * Convierte una matriz OpenCV en un buffer de imagen
   */
  async matToBuffer(mat, width, height) {
    const buffer = Buffer.alloc(width * height);
    mat.data.copy(buffer);
    return await sharp(buffer, {
      raw: {
        width,
        height,
        channels: 1
      }
    })
    .png()
    .toBuffer();
  }

  /**
   * Auto-rotar imagen usando OpenCV.js para an√°lisis avanzado
   */
  async autoRotateImage(input, options = {}) {
    try {
      await this.initialize();

      console.log('üîÑ Iniciando procesamiento de imagen con OpenCV.js NPM...');

      let imageBuffer;
      let originalPath = null;

      // Procesar entrada
      if (typeof input === 'string') {
        originalPath = input;
        imageBuffer = await fs.readFile(input);
        console.log(`üìÅ Analizando: ${path.basename(input)}`);
      } else if (Buffer.isBuffer(input)) {
        imageBuffer = input;
        console.log('üìÅ Procesando buffer de imagen');
      } else {
        throw new Error('Input debe ser una ruta de archivo o Buffer');
      }

      // Obtener metadatos originales
      const metadata = await sharp(imageBuffer).metadata();
      console.log(`üìä Dimensiones originales: ${metadata.width}x${metadata.height}`);

      // Validar y preparar imagen para an√°lisis
      if (!metadata.format || !['jpeg', 'png', 'webp'].includes(metadata.format.toLowerCase())) {
        console.log('‚ö†Ô∏è Formato no √≥ptimo, convirtiendo a PNG...');
        imageBuffer = await sharp(imageBuffer)
          .png()
          .toBuffer();
      }

      // Si solo hay un recibo, procesar normalmente
      const processedBuffer = await sharp(imageBuffer)
        .normalize()
        .sharpen()
        .resize(1200, 1600, {
          fit: 'inside',
          withoutEnlargement: true,
          background: { r: 255, g: 255, b: 255 },
          flatten: true
        })
        .raw()
        .toBuffer({ resolveWithObject: true });

      // Crear Mat de OpenCV desde buffer
      const mat = this.cv.matFromImageData({
        data: new Uint8ClampedArray(processedBuffer.data),
        width: processedBuffer.info.width,
        height: processedBuffer.info.height
      });

      // An√°lisis de orientaci√≥n con OpenCV
      const orientationResult = await this.analyzeOrientation(mat);

      console.log(`üéØ An√°lisis OpenCV: √Ångulo dominante ${orientationResult.dominantAngle}¬∞, L√≠neas detectadas: ${orientationResult.linesDetected}`);

      // Estado inicial de la imagen
      let currentBuffer = imageBuffer;
      let rotationApplied = false;
      let degreesApplied = 0;

      // Si OpenCV detect√≥ que necesita rotaci√≥n
      if (orientationResult.rotationNeeded !== 0) {
        console.log(`üîÑ OpenCV recomienda rotar ${orientationResult.rotationNeeded}¬∞`);

        // Aplicar rotaci√≥n usando Sharp
        currentBuffer = await sharp(imageBuffer)
          .rotate(orientationResult.rotationNeeded)
          .png()
          .toBuffer();

        rotationApplied = true;
        degreesApplied = orientationResult.rotationNeeded;
        console.log(`‚úÖ Rotaci√≥n aplicada: ${degreesApplied}¬∞`);
      } else {
        console.log('‚úÖ No se detect√≥ necesidad de rotaci√≥n');
      }

      // Limpiar memoria de OpenCV
      mat.delete();

      // üßπ PREPROCESAMIENTO ADAPTATIVO BASADO EN TAMA√ëO DE IMAGEN
      console.log('üîß Aplicando preprocesamiento adaptativo para OCR √≥ptimo...');
      
      // An√°lizar la imagen actual
      const currentMetadata = await sharp(currentBuffer).metadata();
      console.log(`üìê Imagen: ${currentMetadata.width}x${currentMetadata.height}, canales: ${currentMetadata.channels}`);
      
      // üéØ DETERMINAR PAR√ÅMETROS ADAPTATIVOS SEG√öN TAMA√ëO
      const imageArea = currentMetadata.width * currentMetadata.height;
      const isSmallReceipt = imageArea < 2000000; // Menor a ~1400x1400px
      
      console.log(`üìä √Årea de imagen: ${imageArea} pixels - ${isSmallReceipt ? 'RECIBO PEQUE√ëO' : 'RECIBO GRANDE'}`);
      
      // Par√°metros adaptativos
      const adaptiveParams = {
        median: isSmallReceipt ? 1 : 2,           // Menos filtrado para recibos peque√±os
        contrast: isSmallReceipt ? 1.05 : 1.10,   // Contraste m√°s suave para peque√±os
        brightness: isSmallReceipt ? 2 : 3,       // Menos brightness para peque√±os
        targetHeight: isSmallReceipt ? 2500 : 2000 // Mayor resoluci√≥n para peque√±os
      };
      
      console.log(`üéõÔ∏è Par√°metros adaptativos:`, adaptiveParams);
      
      // üéØ PIPELINE ADAPTATIVO PARA CADA TIPO DE RECIBO
      let sharpInstance = sharp(currentBuffer);
      
      // 1. Redimensionado adaptativo - m√°s resoluci√≥n para recibos peque√±os
      sharpInstance = sharpInstance.resize(null, adaptiveParams.targetHeight, {
        fit: 'inside',
        withoutEnlargement: false,
        kernel: sharp.kernel.lanczos3  // Kernel de mejor calidad para texto
      });
      
      // 2. Conversi√≥n a escala de grises
      sharpInstance = sharpInstance.greyscale();
      
      // 3. ‚ú® FILTRADO ADAPTATIVO DE RUIDO ‚ú®
      sharpInstance = sharpInstance.median(adaptiveParams.median);
      
      // 4. Contraste adaptativo m√°s suave para recibos peque√±os
      sharpInstance = sharpInstance.linear(adaptiveParams.contrast, adaptiveParams.brightness);
      
      // 5. Normalizaci√≥n para optimizar rango din√°mico
      sharpInstance = sharpInstance.normalize();
      
      console.log(`‚úÖ Preprocesamiento adaptativo completado - optimizado para ${isSmallReceipt ? 'recibo peque√±o' : 'recibo grande'}`);
      
      const finalBuffer = await sharpInstance.png().toBuffer();

      // Validar que la imagen final sea v√°lida
      const finalMetadata = await sharp(finalBuffer).metadata();

      if (!finalMetadata || finalMetadata.size === 0) {
        throw new Error('La imagen procesada est√° corrupta o vac√≠a');
      }

      // Guardar la imagen procesada con un nuevo nombre
      const processedFilePath = originalPath ?
        originalPath.replace(/\.[^.]+$/, '_processed.png') :
        path.join(process.cwd(), 'uploads', `processed_${Date.now()}.png`);

      await fs.writeFile(processedFilePath, finalBuffer);

      const result = {
        success: true,
        rotated: rotationApplied,
        rotationApplied: degreesApplied,
        originalPath: originalPath,
        processedPath: processedFilePath,
        orientationScore: orientationResult.confidence,
        originalDimensions: {
          width: metadata.width,
          height: metadata.height
        },
        finalDimensions: {
          width: finalMetadata.width,
          height: finalMetadata.height
        },
        analysis: {
          openCVResult: orientationResult,
          method: 'opencv-js-npm'
        },
        processingTime: Date.now()
      };

      console.log(`‚úÖ Auto-rotaci√≥n OpenCV completada - ${degreesApplied}¬∞ aplicados`);
      console.log(`üéØ Imagen optimizada para OCR de alta precisi√≥n`);
      return result;

    } catch (error) {
      console.error('‚ùå Error en auto-rotaci√≥n OpenCV:', error.message);

      // Fallback a rotaci√≥n b√°sica si OpenCV falla
      console.log('üîÑ Usando fallback de rotaci√≥n b√°sica...');
      return await this.basicRotationFallback(input);
    }
  }

  /**
   * Analizar orientaci√≥n de documento usando OpenCV
   */
  async analyzeOrientation(mat) {
    try {
      const cv = this.cv;

      // Convertir a escala de grises
      let gray = new cv.Mat();
      cv.cvtColor(mat, gray, cv.COLOR_RGBA2GRAY);

      // Detecci√≥n de bordes usando Canny
      let edges = new cv.Mat();
      cv.Canny(gray, edges, 50, 150, 3, false);

      // Detecci√≥n de l√≠neas usando HoughLinesP
      let lines = new cv.Mat();
      cv.HoughLinesP(edges, lines, 1, Math.PI / 180, 80, 50, 10);

      // Analizar √°ngulos de las l√≠neas detectadas
      let angles = [];
      for (let i = 0; i < lines.rows; ++i) {
        let startPoint = lines.data32S[i * 4];
        let startPointY = lines.data32S[i * 4 + 1];
        let endPoint = lines.data32S[i * 4 + 2];
        let endPointY = lines.data32S[i * 4 + 3];

        let angle = Math.atan2(endPointY - startPointY, endPoint - startPoint) * 180 / Math.PI;
        angles.push(angle);
      }

      // Calcular el √°ngulo dominante
      let dominantAngle = this.calculateDominantAngle(angles);

      // Determinar rotaci√≥n necesaria
      let rotationNeeded = this.determineRotationFromAngle(dominantAngle);

      // Calcular confianza basada en cantidad de l√≠neas
      let confidence = Math.min(angles.length / 20, 1.0); // Normalizar a [0,1]

      // Limpiar memoria
      gray.delete();
      edges.delete();
      lines.delete();

      return {
        dominantAngle: dominantAngle,
        rotationNeeded: rotationNeeded,
        linesDetected: angles.length,
        confidence: confidence,
        success: true
      };

    } catch (error) {
      console.error('Error en an√°lisis OpenCV:', error);
      return {
        dominantAngle: 0,
        rotationNeeded: 0,
        linesDetected: 0,
        confidence: 0,
        success: false,
        error: error.message
      };
    }
  }

  /**
   * Calcular el √°ngulo dominante de un conjunto de √°ngulos
   */
  calculateDominantAngle(angles) {
    if (angles.length === 0) return 0;

    // Normalizar √°ngulos a rango [0, 180)
    let normalizedAngles = angles.map(angle => {
      angle = Math.abs(angle);
      return angle > 90 ? 180 - angle : angle;
    });

    // Crear histograma de √°ngulos (buckets de 5 grados)
    let histogram = {};
    normalizedAngles.forEach(angle => {
      let bucket = Math.round(angle / 5) * 5;
      histogram[bucket] = (histogram[bucket] || 0) + 1;
    });

    // Encontrar el bucket con m√°s ocurrencias
    let maxCount = 0;
    let dominantAngle = 0;
    for (let angle in histogram) {
      if (histogram[angle] > maxCount) {
        maxCount = histogram[angle];
        dominantAngle = parseFloat(angle);
      }
    }

    return dominantAngle;
  }

  /**
   * Determinar rotaci√≥n necesaria basada en el √°ngulo dominante
   */
  determineRotationFromAngle(angle) {
    // Si las l√≠neas est√°n cerca de horizontal (0¬∞), no rotar
    if (angle < 15 || angle > 165) return 0;

    // Si las l√≠neas est√°n cerca de vertical (90¬∞), rotar 270¬∞ (o -90¬∞)
    if (angle > 75 && angle < 105) return 270;

    // Para otros casos, determinar la rotaci√≥n m√°s cercana
    if (angle < 45) return 270; // M√°s cerca de vertical
    if (angle > 135) return 90;  // M√°s cerca de vertical invertida

    return 180; // Probablemente invertido
  }

  /**
   * Fallback b√°sico usando solo Sharp si OpenCV falla
   */
  async loadAndValidateImage(input) {
    let imageBuffer;
    let originalPath = null;

    if (typeof input === 'string') {
      originalPath = input;
      imageBuffer = await fs.readFile(input);
      console.log(`üìÅ Analizando: ${path.basename(input)}`);
    } else if (Buffer.isBuffer(input)) {
      imageBuffer = input;
      console.log('ÔøΩ Procesando buffer de imagen');
    } else {
      throw new Error('Input debe ser una ruta de archivo o Buffer');
    }

    // Validar que el buffer no est√© vac√≠o
    if (!imageBuffer || imageBuffer.length === 0) {
      throw new Error('Imagen inv√°lida: buffer vac√≠o');
    }

    return { imageBuffer, originalPath };
  }

  async prepareImageForAnalysis(imageBuffer, metadata) {
    // Convertir a formato √≥ptimo si es necesario
    if (!metadata.format || !['jpeg', 'png', 'webp'].includes(metadata.format.toLowerCase())) {
      console.log('‚ö†Ô∏è Formato no √≥ptimo, convirtiendo a PNG...');
      imageBuffer = await sharp(imageBuffer)
        .png()
        .toBuffer();
    }

    // Optimizar imagen para OCR
    return await sharp(imageBuffer)
      .normalize() // Normalizar contraste
      .sharpen() // Mejorar nitidez
      .resize(1200, 1600, {
        fit: 'inside',
        withoutEnlargement: true,
        background: { r: 255, g: 255, b: 255 },
        flatten: true // Asegurar fondo blanco
      })
      .raw()
      .toBuffer({ resolveWithObject: true });
  }

  async saveProcessedImage(imageBuffer, originalPath = null) {
    // Generar nombre de archivo para la imagen procesada
    const processedFilePath = originalPath ?
      originalPath.replace(/\.[^.]+$/, '_processed.png') :
      path.join(process.cwd(), 'uploads', `processed_${Date.now()}.png`);

    // Guardar imagen procesada
    await fs.writeFile(processedFilePath, imageBuffer);

    return processedFilePath;
  }

  async basicRotationFallback(input) {
    console.log('üîÑ Ejecutando fallback b√°sico...');

    const { imageBuffer, originalPath } = await this.loadAndValidateImage(input);

    // An√°lisis b√°sico de dimensiones para inferir orientaci√≥n
    const metadata = await sharp(imageBuffer).metadata();
    let rotationNeeded = 0;

    // Heur√≠stica simple: si es m√°s ancho que alto, probablemente necesita rotaci√≥n
    if (metadata.width > metadata.height * 1.5) {
      rotationNeeded = 270; // Rotar 90¬∞ en sentido horario
    }

    let finalBuffer = imageBuffer;
    if (rotationNeeded > 0) {
      finalBuffer = await sharp(imageBuffer)
        .rotate(rotationNeeded)
        .png()
        .toBuffer();
    }

    const finalMetadata = await sharp(finalBuffer).metadata();

    return {
      rotated: rotationNeeded > 0,
      rotationApplied: rotationNeeded,
      buffer: finalBuffer,
      orientationScore: 0.5, // Score b√°sico
      originalDimensions: {
        width: metadata.width,
        height: metadata.height
      },
      finalDimensions: {
        width: finalMetadata.width,
        height: finalMetadata.height
      },
      analysis: {
        openCVResult: { success: false, error: 'Fallback usado' },
        method: 'basic-fallback'
      },
      processingTime: Date.now()
    };
  }

  /**
   * Limpiar recursos (no es necesario con el paquete NPM, pero mantenemos la interfaz)
   */
  async cleanup() {
    console.log('üßπ Cleanup completado (OpenCV NPM se maneja autom√°ticamente)');
  }

  /**
   * Analizar m√∫ltiples im√°genes y obtener estad√≠sticas
   */
  async analyzeRotationStats(imagePaths) {
    const stats = {
      total: imagePaths.length,
      rotationDistribution: { 0: 0, 90: 0, 180: 0, 270: 0 },
      averageConfidence: 0,
      processingTimes: [],
      openCVSuccess: 0
    };

    let totalConfidence = 0;

    console.log(`üìä Analizando ${imagePaths.length} im√°genes con OpenCV.js NPM...`);

    for (const imagePath of imagePaths) {
      try {
        const startTime = Date.now();
        const result = await this.autoRotateImage(imagePath);
        const processingTime = Date.now() - startTime;

        stats.rotationDistribution[result.rotationApplied]++;
        totalConfidence += result.orientationScore;
        stats.processingTimes.push(processingTime);

        if (result.analysis.openCVResult.success) {
          stats.openCVSuccess++;
        }

        console.log(`  ‚úÖ ${path.basename(imagePath)}: ${result.rotationApplied}¬∞ (${processingTime}ms)`);

      } catch (error) {
        console.warn(`‚ö†Ô∏è Error procesando ${imagePath}:`, error.message);
      }
    }

    stats.averageConfidence = totalConfidence / imagePaths.length;
    stats.averageProcessingTime = stats.processingTimes.reduce((a, b) => a + b, 0) / stats.processingTimes.length;
    stats.openCVSuccessRate = (stats.openCVSuccess / imagePaths.length) * 100;

    return stats;
  }

  // ================= üé® MEJORAS DE PROCESAMIENTO DE IMAGEN =================

  /**
   * üåü NORMALIZACI√ìN DE CONTRASTE R√ÅPIDA (< 100ms)
   * Mejora la legibilidad del texto aplicando CLAHE y ajuste de gamma
   * @param {string} imagePath - Ruta de la imagen a procesar
   * @param {Object} options - Configuraci√≥n del procesamiento
   * @returns {Promise<string>} Ruta de la imagen procesada
   */
  async fastContrastNormalization(imagePath, options = {}) {
    const startTime = Date.now();

    try {
      console.log('üåü Aplicando normalizaci√≥n de contraste r√°pida...');

      const {
        clipLimit = 3.0,          // L√≠mite para CLAHE (reducido para ser m√°s r√°pido)
        tileGridSize = 8,         // Tama√±o de grilla (menor = m√°s r√°pido)
        gamma = 1.2,              // Factor gamma para correcci√≥n
        brightness = 1.1          // Factor de brillo
      } = options;

      // Leer imagen con Sharp (m√°s r√°pido que OpenCV para operaciones b√°sicas)
      const buffer = await fs.readFile(imagePath);

      // Aplicar mejoras r√°pidas con Sharp
      const enhancedBuffer = await sharp(buffer)
        .normalize()                    // Normalizaci√≥n autom√°tica
        .modulate({
          brightness: brightness,       // Ajuste de brillo
          saturation: 0.8              // Reducir saturaci√≥n para mejorar OCR
        })
        .sharpen({                      // Enfoque ligero
          sigma: 1,
          flat: 1,
          jagged: 2
        })
        .jpeg({ quality: 95 })          // Alta calidad para OCR
        .toBuffer();

      // Generar nombre de archivo para la imagen procesada
      const parsedPath = path.parse(imagePath);
      const outputPath = path.join(parsedPath.dir, `${parsedPath.name}_contrast${parsedPath.ext}`);

      await fs.writeFile(outputPath, enhancedBuffer);

      const processingTime = Date.now() - startTime;
      console.log(`‚úÖ Contraste normalizado en ${processingTime}ms: ${path.basename(outputPath)}`);

      return outputPath;

    } catch (error) {
      console.error('‚ùå Error en normalizaci√≥n de contraste:', error);
      return imagePath; // Devolver imagen original si falla
    }
  }

  /**
   * üîÑ DETECCI√ìN DE ORIENTACI√ìN MEJORADA
   * Combina detecci√≥n por texto y an√°lisis de contenido
   * @param {string} imagePath - Ruta de la imagen
   * @returns {Promise<number>} √Ångulo de rotaci√≥n recomendado (0, 90, 180, 270)
   */
  async improvedOrientationDetection(imagePath) {
    const startTime = Date.now();

    try {
      console.log('üîÑ Detectando orientaci√≥n mejorada...');

      // Usar el m√©todo existente como base
      const existingResult = await this.autoRotateImage(imagePath);

      // Si la confianza es alta, usar el resultado existente
      if (existingResult.orientationScore > 0.7) {
        console.log(`‚úÖ Orientaci√≥n detectada (alta confianza): ${existingResult.rotationApplied}¬∞`);
        return existingResult.rotationApplied;
      }

      // An√°lisis adicional para casos dif√≠ciles
      await this.initialize();
      const img = this.cv.imread(imagePath);

      // Detectar l√≠neas horizontales y verticales
      const gray = new this.cv.Mat();
      this.cv.cvtColor(img, gray, this.cv.COLOR_RGBA2GRAY);

      // Detectar bordes
      const edges = new this.cv.Mat();
      this.cv.Canny(gray, edges, 50, 150);

      // Detectar l√≠neas con HoughLines
      const lines = new this.cv.Mat();
      this.cv.HoughLines(edges, lines, 1, Math.PI / 180, 100);

      // Analizar √°ngulos de las l√≠neas
      let horizontalLines = 0;
      let verticalLines = 0;

      for (let i = 0; i < lines.rows; i++) {
        const theta = lines.data32F[i * 2 + 1];
        const angle = (theta * 180) / Math.PI;

        if (Math.abs(angle) < 15 || Math.abs(angle - 180) < 15) {
          horizontalLines++;
        } else if (Math.abs(angle - 90) < 15) {
          verticalLines++;
        }
      }

      // Limpiar memoria
      img.delete();
      gray.delete();
      edges.delete();
      lines.delete();

      // Determinar orientaci√≥n basada en an√°lisis de l√≠neas
      let recommendedRotation = 0;
      if (verticalLines > horizontalLines * 1.5) {
        recommendedRotation = 90; // Probablemente necesita rotaci√≥n
      }

      const processingTime = Date.now() - startTime;
      console.log(`‚úÖ Orientaci√≥n mejorada detectada en ${processingTime}ms: ${recommendedRotation}¬∞`);

      return recommendedRotation;

    } catch (error) {
      console.error('‚ùå Error en detecci√≥n de orientaci√≥n mejorada:', error);
      return 0; // Sin rotaci√≥n si falla
    }
  }

  /**
   * üßπ LIMPIEZA DE RUIDO B√ÅSICA
   * Elimina ruido, manchas y artefactos que interfieren con el OCR
   * @param {string} imagePath - Ruta de la imagen
   * @param {Object} options - Configuraci√≥n de limpieza
   * @returns {Promise<string>} Ruta de la imagen limpia
   */
  async basicNoiseReduction(imagePath, options = {}) {
    const startTime = Date.now();

    try {
      console.log('üßπ Aplicando limpieza de ruido b√°sica...');

      const {
        denoiseStrength = 3,      // Fuerza de reducci√≥n de ruido (1-10)
        morphKernelSize = 2,      // Tama√±o kernel morfol√≥gico
        blurRadius = 1            // Radio de desenfoque gaussiano
      } = options;

      await this.initialize();

      // Leer imagen
      const img = this.cv.imread(imagePath);
      const processed = new this.cv.Mat();

      // Convertir a escala de grises
      this.cv.cvtColor(img, processed, this.cv.COLOR_RGBA2GRAY);

      // 1. Reducci√≥n de ruido con filtro bilateral (preserva bordes)
      const denoised = new this.cv.Mat();
      this.cv.bilateralFilter(processed, denoised, 9, 75, 75);

      // 2. Operaciones morfol√≥gicas para limpiar texto
      const kernel = this.cv.getStructuringElement(
        this.cv.MORPH_RECT,
        new this.cv.Size(morphKernelSize, morphKernelSize)
      );

      const morphed = new this.cv.Mat();
      // Closing para cerrar gaps en letras
      this.cv.morphologyEx(denoised, morphed, this.cv.MORPH_CLOSE, kernel);

      // 3. Ligero desenfoque gaussiano para suavizar
      const final = new this.cv.Mat();
      this.cv.GaussianBlur(morphed, final, new this.cv.Size(blurRadius * 2 + 1, blurRadius * 2 + 1), 0);

      // 4. Umbralizaci√≥n adaptativa para mejorar contraste del texto
      const binary = new this.cv.Mat();
      this.cv.adaptiveThreshold(
        final, binary, 255,
        this.cv.ADAPTIVE_THRESH_GAUSSIAN_C,
        this.cv.THRESH_BINARY,
        11, 2
      );

      // Generar nombre de archivo para la imagen limpia
      const parsedPath = path.parse(imagePath);
      const outputPath = path.join(parsedPath.dir, `${parsedPath.name}_clean${parsedPath.ext}`);

      // Guardar imagen procesada
      this.cv.imwrite(outputPath, binary);

      // Limpiar memoria
      img.delete();
      processed.delete();
      denoised.delete();
      kernel.delete();
      morphed.delete();
      final.delete();
      binary.delete();

      const processingTime = Date.now() - startTime;
      console.log(`‚úÖ Ruido eliminado en ${processingTime}ms: ${path.basename(outputPath)}`);

      return outputPath;

    } catch (error) {
      console.error('‚ùå Error en limpieza de ruido:', error);
      return imagePath; // Devolver imagen original si falla
    }
  }

  /**
   * üöÄ PROCESAMIENTO COMPLETO OPTIMIZADO
   * Aplica todas las mejoras en secuencia optimizada
   * @param {string} imagePath - Ruta de la imagen original
   * @param {Object} options - Configuraci√≥n del procesamiento
   * @returns {Promise<Object>} Resultado del procesamiento completo
   */
  async optimizedFullProcessing(imagePath, options = {}) {
    const startTime = Date.now();

    try {
      console.log('üöÄ Iniciando procesamiento completo optimizado...');

      const {
        enableContrast = true,
        enableOrientation = true,
        enableNoiseReduction = true,
        keepIntermediateFiles = false
      } = options;

      let currentPath = imagePath;
      const results = {
        originalPath: imagePath,
        finalPath: imagePath,
        steps: [],
        totalTime: 0,
        improvements: []
      };

      // 1. Normalizaci√≥n de contraste (m√°s r√°pida primero)
      if (enableContrast) {
        const stepStart = Date.now();
        currentPath = await this.fastContrastNormalization(currentPath);
        const stepTime = Date.now() - stepStart;
        results.steps.push({ step: 'contrast', time: stepTime, path: currentPath });
        results.improvements.push('contrast_normalized');
      }

      // 2. Limpieza de ruido
      if (enableNoiseReduction) {
        const stepStart = Date.now();
        currentPath = await this.basicNoiseReduction(currentPath);
        const stepTime = Date.now() - stepStart;
        results.steps.push({ step: 'noise_reduction', time: stepTime, path: currentPath });
        results.improvements.push('noise_reduced');
      }

      // 3. Detecci√≥n de orientaci√≥n (al final para trabajar con imagen limpia)
      if (enableOrientation) {
        const stepStart = Date.now();
        const rotation = await this.improvedOrientationDetection(currentPath);
        const stepTime = Date.now() - stepStart;

        if (rotation !== 0) {
          // Aplicar rotaci√≥n si es necesaria
          const rotatedPath = await this.autoRotateImage(currentPath, { forceAngle: rotation });
          currentPath = rotatedPath.outputPath || rotatedPath.path || currentPath;
          results.improvements.push(`rotated_${rotation}deg`);
        }

        results.steps.push({ step: 'orientation', time: stepTime, rotation, path: currentPath });
      }

      // Limpiar archivos intermedios si no se requieren
      if (!keepIntermediateFiles) {
        for (const step of results.steps) {
          if (step.path !== imagePath && step.path !== currentPath) {
            try {
              await fs.unlink(step.path);
            } catch (e) {
              // Ignorar errores de limpieza
            }
          }
        }
      }

      return results;
    } catch (error) {
      console.error(`‚ùå Error en processWithSteps: ${error.message}`);
      throw error;
    }
  }

  /**
   * üìè Calcula la altura √≥ptima para OCR basada en las caracter√≠sticas de la imagen
   * @param {Object} metadata - Metadatos de la imagen de Sharp
   * @returns {number} - Altura objetivo en p√≠xeles
   */
  calculateOptimalHeight(metadata) {
    const { width, height } = metadata;
    const aspectRatio = width / height;
    
    // Para im√°genes muy anchas (m√∫ltiples recibos), usar m√°s resoluci√≥n
    if (aspectRatio > 1.5) {
      return 2400; // M√°s resoluci√≥n para im√°genes anchas
    }
    
    // Para recibos individuales altos y delgados
    if (aspectRatio < 0.8) {
      return 2200; // Resoluci√≥n alta para recibos individuales
    }
    
    // Para im√°genes cuadradas o proporci√≥n normal
    return 2000; // Resoluci√≥n est√°ndar
  }

  /**
   * üéõÔ∏è Calcula el factor de contraste √≥ptimo basado en las caracter√≠sticas de la imagen
   * @param {Object} metadata - Metadatos de la imagen de Sharp
   * @returns {number} - Factor de contraste entre 1.0 y 1.5
   */
  calculateOptimalContrast(metadata) {
    const { width, height } = metadata;
    const totalPixels = width * height;
    
    // Para im√°genes m√°s peque√±as, aplicar m√°s contraste
    if (totalPixels < 2000000) { // Menos de 2MP
      return 1.3;
    }
    
    // Para im√°genes muy grandes, contraste moderado
    if (totalPixels > 6000000) { // M√°s de 6MP
      return 1.15;
    }
    
    // Para el rango medio, contraste balanceado
    return 1.2;
  }

  /**
   * üîç Calcula el sigma de enfoque √≥ptimo basado en las caracter√≠sticas de la imagen
   * @param {Object} metadata - Metadatos de la imagen de Sharp
   * @returns {number} - Valor sigma entre 0.5 y 2.0
   */
  calculateOptimalSharpen(metadata) {
    const { width, height } = metadata;
    const aspectRatio = width / height;
    
    // Para im√°genes muy anchas (posible m√∫ltiples recibos), enfoque moderado
    if (aspectRatio > 1.5) {
      return 0.8;
    }
    
    // Para recibos individuales, enfoque m√°s agresivo
    if (aspectRatio < 0.8) {
      return 1.2;
    }
    
    // Para proporci√≥n normal, enfoque balanceado
    return 1.0;

      results.finalPath = currentPath;
      results.totalTime = Date.now() - startTime;

      console.log(`‚úÖ Procesamiento completo terminado en ${results.totalTime}ms`);
      console.log(`üéØ Mejoras aplicadas: ${results.improvements.join(', ')}`);

      return results;

    } catch (error) {
      console.error('‚ùå Error en procesamiento completo:', error);
      return {
        originalPath: imagePath,
        finalPath: imagePath,
        error: error.message,
        totalTime: Date.now() - startTime
      };
    }
  }

// Exportar la clase en lugar de una instancia
export default ImageProcessor;
